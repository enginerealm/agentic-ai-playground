{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be13a530",
   "metadata": {},
   "source": [
    "### Profile-to-Chat: Exploring Yourself Through LLMs\n",
    "\n",
    "In this lab, we'll upload my LinkedIn profile and summary to a LLM, then interact with it through a chat interface to ask questions about myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a40c27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aeba9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_client = OpenAI()\n",
    "gemini_openai_client = OpenAI(api_key=os.getenv(\"GEMINI_API_KEY\"), base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "openai_model_name = \"gpt-4o-mini\"\n",
    "gemini_model_name = \"gemini-2.0-flash\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e312c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_pdf_reader = PdfReader(\"me/My_LinkedIn_Profile.pdf\")\n",
    "linkedIn_profile_text = \"\"\n",
    "\n",
    "for pages in linkedin_pdf_reader.pages:\n",
    "    page_text = pages.extract_text()\n",
    "    if page_text:\n",
    "        linkedIn_profile_text += page_text\n",
    "\n",
    "print(linkedIn_profile_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3580b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\") as summary_file:\n",
    "    summary_text = summary_file.read()\n",
    "\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef20235",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Varun Kakuste\"\n",
    "first_name = \"Varun\"\n",
    "\n",
    "print(name)\n",
    "print(first_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are acting as {name}. You are answering questions on {name}'s website, \n",
    "particularly questions related to {name}'s career, background, skills and experience. \n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \n",
    "If you don't know the answer, say so.\n",
    "\n",
    "## Summary:\n",
    "{summary_text}\n",
    "\n",
    "## LinkedIn Profile:\n",
    "{linkedIn_profile_text}\n",
    "\n",
    "With this context, please chat with the user, always staying in character as {name}.\n",
    "\"\"\"\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2001293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llm(user_prompt, history):\n",
    "\n",
    "    system = system_prompt\n",
    "\n",
    "    if \"gibberish\" in user_prompt:\n",
    "        system =\"You are AI Assistant, reply with some gibberish\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "\n",
    "    if user_prompt in history:\n",
    "        history.pop()\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    response = openai_client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "\n",
    "    llm_response = response.choices[0].message.content\n",
    "\n",
    "    return llm_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat_with_llm, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249eaf8",
   "metadata": {},
   "source": [
    "#### What's next?\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer \n",
    "2. Be able to rerun if the answer fails evaluation \n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b98201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"\"\"\n",
    "You are an expert evaluator AI.\n",
    "\n",
    "Your job is to review a conversation between a visitor and an AI agent that represents a real person named {name}. The AI agent is embedded on {first_name}'s personal website and answers questions about him — using his LinkedIn profile and professional summary as the knowledge base.\n",
    "\n",
    "{first_name}'s LinkedIn profile is {linkedIn_profile_text} and summary is {summary_text}.\n",
    "\n",
    "Visitors may include investors, hiring managers, founders, recruiters, or others curious about {first_name}'s background, expertise, and interests.\n",
    "\n",
    "You will be shown the entire conversation history, including the visitor’s questions and the AI agent’s responses.\n",
    "\n",
    "With this context, please evaluate only the most recent message from the AI agent.\n",
    "\n",
    "Return your output strictly in this JSON format:\n",
    "- Respond with `is_acceptable: True` if the message is acceptable, and provide an empty `feedback` string i.e., do not provide any feedback if the message is acceptable.\n",
    "- Respond with `is_acceptable: False` if the message is not acceptable, and include a brief explanation in the `feedback` string.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(evaluator_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d771395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_for_evaluator(history, user_prompt, agent_response):\n",
    "    user_prompt = f\"\"\"\n",
    "    Conversation history:\n",
    "    {history}\n",
    "\n",
    "    Latest User prompt:\n",
    "    {user_prompt}\n",
    "\n",
    "    Latest Agent response:\n",
    "    {agent_response}\n",
    "    \"\"\"\n",
    "\n",
    "    return user_prompt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b3f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent_response(history, latest_user_prompt, latest_agent_response) -> Evaluation:\n",
    "\n",
    "    # If the latest user prompt is in the history, remove it\n",
    "    if latest_user_prompt in history:\n",
    "        history.pop()\n",
    "\n",
    "    user_prompt = get_user_prompt_for_evaluator(history, latest_user_prompt, latest_agent_response)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    response = gemini_openai_client.chat.completions.parse(model=gemini_model_name, messages=messages, response_format=Evaluation)\n",
    "\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"What do you do in your free time? Respond in pig latin\"}]\n",
    "response = openai_client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eed3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent_response(messages, \"What do you do in your free time?\", reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e94418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rerun_agent_system_prompt(latest_agent_response, evaluator_feedback):\n",
    "    updated_system_prompt = f\"\"\"\n",
    "    {system_prompt} \\n\\n\n",
    "    ## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\n",
    "    ## Your attempted answer:\\n{latest_agent_response}\\n\\n\n",
    "    ## Reason for rejection:\\n{evaluator_feedback}\\n\\n\n",
    "    \"\"\"\n",
    "    return updated_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "004ed7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun_agent(history, latest_user_prompt, latest_agent_response, evaluator_feedback):\n",
    "    rerun_agent_system_prompt = get_rerun_agent_system_prompt(latest_agent_response, evaluator_feedback)\n",
    "    messages = [{\"role\": \"system\", \"content\": rerun_agent_system_prompt}] + history + [{\"role\": \"user\", \"content\": latest_user_prompt}]\n",
    "    response = openai_client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "    new_reply = response.choices[0].message.content\n",
    "    return new_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26b0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent(user_prompt, history):\n",
    "    if \"patent\" in user_prompt:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    response = openai_client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate_agent_response(history, user_prompt, reply)\n",
    "\n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"The response is acceptable\")\n",
    "    else:\n",
    "        print(\"The response is not acceptable\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun_agent(history, user_prompt, reply, evaluation.feedback)\n",
    "\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat_with_agent, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
