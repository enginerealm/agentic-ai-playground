{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6d9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7be7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: /Users/varunkakuste/Documents/udemy/ai/hands_on_practice/agentic-ai-playground\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Current working dir:\", Path().resolve().parent)\n",
    "dotenv_path = Path(\"../.env\")\n",
    "print(dotenv_path.exists())\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 400 - {'error': {'message': \"The requested model 'gemini-2.0-flash' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}. (request_id: req_f63f61a824404679f8d9dcf5ee15754c)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"The requested model 'gemini-2.0-flash' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mgemini-2.0-flash\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m agent = Agent(name=\u001b[33m\"\u001b[39m\u001b[33mMath_Tutor\u001b[39m\u001b[33m\"\u001b[39m, instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are a Math Tutor\u001b[39m\u001b[33m\"\u001b[39m, model=model_name)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m runner = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, \u001b[33m\"\u001b[39m\u001b[33mWhat is 2 + 2? Only give direct result\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(runner)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(runner.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/agents/run.py:206\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    205\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    207\u001b[39m     starting_agent,\n\u001b[32m    208\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    209\u001b[39m     context=context,\n\u001b[32m    210\u001b[39m     max_turns=max_turns,\n\u001b[32m    211\u001b[39m     hooks=hooks,\n\u001b[32m    212\u001b[39m     run_config=run_config,\n\u001b[32m    213\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    214\u001b[39m     session=session,\n\u001b[32m    215\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/agents/run.py:412\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m logger.debug(\n\u001b[32m    408\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    409\u001b[39m )\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    413\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    414\u001b[39m             starting_agent,\n\u001b[32m    415\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    416\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    417\u001b[39m             copy.deepcopy(prepared_input),\n\u001b[32m    418\u001b[39m             context_wrapper,\n\u001b[32m    419\u001b[39m         ),\n\u001b[32m    420\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    421\u001b[39m             agent=current_agent,\n\u001b[32m    422\u001b[39m             all_tools=all_tools,\n\u001b[32m    423\u001b[39m             original_input=original_input,\n\u001b[32m    424\u001b[39m             generated_items=generated_items,\n\u001b[32m    425\u001b[39m             hooks=hooks,\n\u001b[32m    426\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    427\u001b[39m             run_config=run_config,\n\u001b[32m    428\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    429\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    430\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    431\u001b[39m         ),\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    435\u001b[39m         agent=current_agent,\n\u001b[32m    436\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    445\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/agents/run.py:960\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    958\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    961\u001b[39m     agent,\n\u001b[32m    962\u001b[39m     system_prompt,\n\u001b[32m    963\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    964\u001b[39m     output_schema,\n\u001b[32m    965\u001b[39m     all_tools,\n\u001b[32m    966\u001b[39m     handoffs,\n\u001b[32m    967\u001b[39m     context_wrapper,\n\u001b[32m    968\u001b[39m     run_config,\n\u001b[32m    969\u001b[39m     tool_use_tracker,\n\u001b[32m    970\u001b[39m     previous_response_id,\n\u001b[32m    971\u001b[39m     prompt_config,\n\u001b[32m    972\u001b[39m )\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    975\u001b[39m     agent=agent,\n\u001b[32m    976\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    985\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    986\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/agents/run.py:1121\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\u001b[39m\n\u001b[32m   1118\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m   1119\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1122\u001b[39m     system_instructions=system_prompt,\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1124\u001b[39m     model_settings=model_settings,\n\u001b[32m   1125\u001b[39m     tools=all_tools,\n\u001b[32m   1126\u001b[39m     output_schema=output_schema,\n\u001b[32m   1127\u001b[39m     handoffs=handoffs,\n\u001b[32m   1128\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1129\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1130\u001b[39m     ),\n\u001b[32m   1131\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1132\u001b[39m     prompt=prompt_config,\n\u001b[32m   1133\u001b[39m )\n\u001b[32m   1135\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/agents/models/openai_responses.py:83\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     84\u001b[39m             system_instructions,\n\u001b[32m     85\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     86\u001b[39m             model_settings,\n\u001b[32m     87\u001b[39m             tools,\n\u001b[32m     88\u001b[39m             output_schema,\n\u001b[32m     89\u001b[39m             handoffs,\n\u001b[32m     90\u001b[39m             previous_response_id,\n\u001b[32m     91\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     92\u001b[39m             prompt=prompt,\n\u001b[32m     93\u001b[39m         )\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m     96\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/agents/models/openai_responses.py:267\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream, prompt)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     logger.debug(\n\u001b[32m    258\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling LLM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    259\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(list_input,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m,\u001b[38;5;250m \u001b[39mensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrevious response id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_response_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    268\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    269\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    270\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    271\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    272\u001b[39m     include=include,\n\u001b[32m    273\u001b[39m     tools=converted_tools.tools,\n\u001b[32m    274\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(prompt),\n\u001b[32m    275\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    276\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    277\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    278\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    279\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    280\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    281\u001b[39m     stream=stream,\n\u001b[32m    282\u001b[39m     extra_headers={**_HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    283\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    284\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    285\u001b[39m     text=response_format,\n\u001b[32m    286\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    287\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    288\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    289\u001b[39m     **(model_settings.extra_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    290\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:2000\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1967\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1968\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1969\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1998\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1999\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2001\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2002\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2003\u001b[39m             {\n\u001b[32m   2004\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2005\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2006\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2007\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2008\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2009\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2010\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2011\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2012\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2013\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2014\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2015\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2016\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2017\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2018\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2019\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2020\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2021\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2022\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2023\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2024\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2025\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2026\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2027\u001b[39m             },\n\u001b[32m   2028\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2029\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2030\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2031\u001b[39m         ),\n\u001b[32m   2032\u001b[39m         options=make_request_options(\n\u001b[32m   2033\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2034\u001b[39m         ),\n\u001b[32m   2035\u001b[39m         cast_to=Response,\n\u001b[32m   2036\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2037\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2038\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/openai/_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/udemy/ai/hands_on_practice/agentic-ai-playground/.venv/lib/python3.12/site-packages/openai/_base_client.py:1591\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1588\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1590\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"The requested model 'gemini-2.0-flash' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "model_name = \"gemini-2.0-flash\"\n",
    "agent = Agent(name=\"Math_Tutor\", instructions=\"You are a Math Tutor\", model=model_name, api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "runner = await Runner.run(agent, \"What is 2 + 2? Only give direct result\")\n",
    "\n",
    "print(runner)\n",
    "print(runner.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"Math_Tutor\"):\n",
    "    runner = await Runner.run(agent, \"What is 2 + 2? Only give direct result\")\n",
    "    print(runner)\n",
    "    print(runner.final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
