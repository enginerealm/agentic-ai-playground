{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e5e0ba",
   "metadata": {},
   "source": [
    "# How to use Guardrails? and Structured output\n",
    "\n",
    "We will implement a Guardrail to check if Name is present in the user_prompt, if so, it should fail otherwise pass and return structured output\n",
    "\n",
    "1. Input Guardrail\n",
    "2. Output Gaurdrail\n",
    "\n",
    "| Guardrail Type   | Purpose               | Example Trigger                 |\n",
    "| ---------------- | --------------------- | ------------------------------- |\n",
    "| Input Guardrail  | Before agent responds | User input contains banned name |\n",
    "| Output Guardrail | After agent responds  | Agent response includes a name  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "849c23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, trace, OpenAIChatCompletionsModel, input_guardrail, output_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "gemini_client = AsyncOpenAI(api_key=os.getenv(\"GEMINI_API_KEY\"), base_url=os.getenv(\"GEMINI_BASE_URL\"))\n",
    "\n",
    "gemini_model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "gemini_model = OpenAIChatCompletionsModel(openai_client=gemini_client, model=gemini_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a4ffb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGuardrailResponse(BaseModel):\n",
    "    is_name_present: bool\n",
    "    name: str\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cc83d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameCheckGuardrailResponse(BaseModel):    \n",
    "    is_name_present: bool\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "54db961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiResponseFormat(BaseModel):\n",
    "    is_name_present: bool\n",
    "    name: str\n",
    "    # Note: Below does not work with Gemini Model but works with OpenAI Model \n",
    "    # Error: BadRequestError: Error code: 400 \n",
    "    # - [{'error': {'code': 400, 'message': \"Unable to submit request because one or more response schemas didn't specify the schema type field.\n",
    "    \n",
    "    # name: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c773c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_check_guardrail_instructions = \"You are a guardrail agent that checks if the name is present in the user prompt. If it is, it should fail otherwise pass and return structured output.\"\n",
    "name_check_guardrail_agent = Agent(name=\"name_check_guardrail_agent\", instructions=name_check_guardrail_instructions, model=gemini_model, output_type=NameCheckGuardrailResponse)\n",
    "\n",
    "name_check_guardrail_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ff91d",
   "metadata": {},
   "source": [
    "# Input Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8149edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@input_guardrail()\n",
    "async def name_check_guardrail_function(ctx, agent, user_prompt):\n",
    "    response = await Runner.run(name_check_guardrail_agent, user_prompt, context=ctx.context)\n",
    "    is_name_present_in_prompt = response.final_output.is_name_present\n",
    "    return GuardrailFunctionOutput(\n",
    "        tripwire_triggered=is_name_present_in_prompt,\n",
    "        output_info={\"found_name\": response.final_output.name}\n",
    "    )\n",
    "\n",
    "\n",
    "# print(name_check_guardrail_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3dc9504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gemini_model_agent = Agent(name=\"test_gemini_model_agent\", instructions=\"You are a test agent that uses the Gemini model to generate a response.\", model=gemini_model, output_type=TestGuardrailResponse, input_guardrails=[name_check_guardrail_function])\n",
    "\n",
    "user_prompt_with_name = \"Is Sundar Pichai your CEO?\"\n",
    "user_prompt_without_name = \"Is AI your CEO?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98913c11",
   "metadata": {},
   "source": [
    "### Below will fail with `InputGuardrailTripwireTriggered: Guardrail InputGuardrail triggered tripwire` error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below will fail with `InputGuardrailTripwireTriggered: Guardrail InputGuardrail triggered tripwire` error\n",
    "\n",
    "response_from_test_gemini_model_agent = await Runner.run(test_gemini_model_agent, user_prompt_with_name)\n",
    "\n",
    "print(response_from_test_gemini_model_agent.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee69765",
   "metadata": {},
   "source": [
    "### Customize Response Instead of Crashing with InputGuardrailTripwireTriggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeecf029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But Input Guardrail returns output_info as a dictionary, so we should error message instead of error stacktrace\n",
    "# Customize Response Instead of Crashing\n",
    "\n",
    "try:\n",
    "    user_prompt = \"Is Sundar Pichai your CEO?\"\n",
    "    response_from_test_gemini_model_agent = await Runner.run(test_gemini_model_agent, user_prompt)\n",
    "    print(response_from_test_gemini_model_agent.final_output)\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    name_found_in_prompt = e.guardrail_result.output.output_info[\"found_name\"]\n",
    "    print(f\"ERROR: Sorry, you can't mention personal names like '{name_found_in_prompt}' in this assistant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working user_prompt which doesn't have name in it\n",
    "\n",
    "response_from_test_gemini_model_agent = await Runner.run(test_gemini_model_agent, user_prompt_without_name)\n",
    "\n",
    "print(response_from_test_gemini_model_agent.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3906e",
   "metadata": {},
   "source": [
    "# Output Gaurdrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5224a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfanityCheckGuardrailResponse(BaseModel):\n",
    "    is_profanity_present: bool\n",
    "    profanity: str\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecad9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity_check_guardrail_instructions = \"You are a guardrail agent that checks if the profanity is present in the user prompt. If it is, it should fail otherwise pass and return structured output.\"\n",
    "profanity_check_guardrail_agent = Agent(\n",
    "    name=\"profanity_check_guardrail_agent\", \n",
    "    instructions=profanity_check_guardrail_instructions, \n",
    "    model=gemini_model, \n",
    "    output_type=ProfanityCheckGuardrailResponse\n",
    ")\n",
    "\n",
    "print(profanity_check_guardrail_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "72adaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output_guardrail()\n",
    "async def profanity_check_guardrail_function(ctx, agent, user_prompt):\n",
    "    response = await Runner.run(profanity_check_guardrail_agent, user_prompt, context=ctx.context)\n",
    "    is_profanity_present_in_response = response.final_output.is_profanity_present\n",
    "    return GuardrailFunctionOutput(\n",
    "        tripwire_triggered=is_profanity_present_in_response, \n",
    "        output_info={\"found_profanity\": response.final_output.profanity}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71ca29",
   "metadata": {},
   "source": [
    "### Below will fail with `OutputGuardrailTripwireTriggered: Guardrail OutputGuardrail triggered tripwire` error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1397fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below will fail with `OutputGuardrailTripwireTriggered: Guardrail OutputGuardrail triggered tripwire` error\n",
    "\n",
    "gemini_returns_profanity_agent = Agent(\n",
    "    name=\"gemini_returns_profanity_agent\",\n",
    "    instructions=\"You are an assistant that returns profanity in the response all the time.\",\n",
    "    model=gemini_model,\n",
    "    output_guardrails=[profanity_check_guardrail_function]\n",
    ")\n",
    "\n",
    "user_prompt = \"I want to respond to someone who is a jerk so that they can feel bad about themselves.\"\n",
    "\n",
    "response = await Runner.run(gemini_returns_profanity_agent, user_prompt)\n",
    "\n",
    "print(response.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8e66d",
   "metadata": {},
   "source": [
    "### Customize Response Instead of Crashing with OutputGuardrailTripwireTriggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7022e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = await Runner.run(gemini_returns_profanity_agent, user_prompt)\n",
    "    print(response.final_output)\n",
    "except OutputGuardrailTripwireTriggered as e:\n",
    "    found_profanity_in_agents_response = e.guardrail_result.output.output_info[\"found_profanity\"]\n",
    "    print(f\"ERROR: Agent responded with profanity: {found_profanity_in_agents_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
